import numpy as np

# Activation function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Derivative for backpropagation
def sigmoid_derivative(x):
    return x * (1 - x)

# Input (OR gate)
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

# Output (OR logic)
y = np.array([[0], [1], [1], [1]])

# Initialize weights randomly
weights = np.random.rand(2, 1)
bias = np.random.rand(1)

# Train the network
for _ in range(10000):
    # Feedforward
    z = np.dot(X, weights) + bias
    output = sigmoid(z)

    # Backpropagation
    error = y - output
    updates = error * sigmoid_derivative(output)
    weights += np.dot(X.T, updates)
    bias += np.sum(updates)

# Final output
print("Predicted output:")
print(np.round(output, 2))
